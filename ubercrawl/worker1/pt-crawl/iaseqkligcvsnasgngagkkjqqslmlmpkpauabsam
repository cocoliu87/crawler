iaseqkligcvsnasgngagkkjqqslmlmpkpauabsam length 6 239497 page 10049 This article is about a branch of mathematics. For the Swedish band, see Abstrakt Algebra. "Modern algebra" redirects here. For van der Waerden's book, see Moderne Algebra. Branch of mathematics The permutations of the Rubik's Cube form a group, a fundamental concept within abstract algebra. In mathematics, more specifically algebra, abstract algebra or modern algebra is the study of algebraic structures.[1] Algebraic structures include groups, rings, fields, modules, vector spaces, lattices, and algebras over a field. The term abstract algebra was coined in the early 20th century to distinguish it from older parts of algebra, and more specifically from elementary algebra, the use of variables to represent numbers in computation and reasoning. The abstract perspective on algebra has become so fundamental to advanced mathematics that it is simply called "algebra", while the term "abstract algebra" is seldom used except in pedagogy. Algebraic structures, with their associated homomorphisms, form mathematical categories. Category theory gives a unified framework to study properties and constructions that are similar for various structures. Universal algebra is a related subject that studies types of algebraic structures as single objects. For example, the structure of groups is a single object in universal algebra, which is called the variety of groups. History[edit] Before the nineteenth century, algebra was defined as the study of polynomials.[2] Abstract algebra came into existence during the nineteenth century as more complex problems and solution methods developed. Concrete problems and examples came from number theory, geometry, analysis, and the solutions of algebraic equations. Most theories that are now recognized as parts of abstract algebra started as collections of disparate facts from various branches of mathematics, acquired a common theme that served as a core around which various results were grouped, and finally became unified on a basis of a common set of concepts. This unification occurred in the early decades of the 20th century and resulted in the formal axiomatic definitions of various algebraic structures such as groups, rings, and fields.[3] This historical development is almost the opposite of the treatment found in popular textbooks, such as van der Waerden's Moderne Algebra,[4] which start each chapter with a formal definition of a structure and then follow it with concrete examples.[5] Elementary algebra[edit] Main article: History of algebra The study of polynomial equations or algebraic equations has a long history. c. 1700 BC, the Babylonians were able to solve quadratic equations specified as word problems. This word problem stage is classified as rhetorical algebra and was the dominant approach up to the 16th century. Muhammad ibn Mūsā al-Khwārizmī originated the word "algebra" in 830 AD, but his work was entirely rhetorical algebra. Fully symbolic algebra did not appear until François Viète's 1591 New Algebra, and even this had some spelled out words that were given symbols in Descartes's 1637 La Géométrie.[6] The formal study of solving symbolic equations led Leonhard Euler to accept what were then considered "nonsense" roots such as negative numbers and imaginary numbers, in the late 18th century.[7] However, European mathematicians, for the most part, resisted these concepts until the middle of the 19th century.[8] George Peacock's 1830 Treatise of Algebra was the first attempt to place algebra on a strictly symbolic basis. He distinguished a new symbolical algebra, distinct from the old arithmetical algebra. Whereas in arithmetical algebra a − b {\displaystyle a-b} is restricted to a ≥ b {\displaystyle a\geq b} , in symbolical algebra all rules of operations hold with no restrictions. Using this Peacock could show laws such as ( − a ) ( − b ) = a b {\displaystyle (-a)(-b)=ab} , by letting a = 0 , c = 0 {\displaystyle a=0,c=0} in ( a − b ) ( c − d ) = a c + b d − a d − b c {\displaystyle (a-b)(c-d)=ac+bd-ad-bc} . Peacock used what he termed the principle of the permanence of equivalent forms to justify his argument, but his reasoning suffered from the problem of induction.[9] For example, a b = a b {\displaystyle {\sqrt {a}}{\sqrt {b}}={\sqrt {ab}}} holds for the nonnegative real numbers, but not for general complex numbers. Early group theory[edit] Main article: History of group theory Several areas of mathematics led to the study of groups. Lagrange's 1770 study of the solutions of the quintic equation led to the Galois group of a polynomial. Gauss's 1801 study of Fermat's little theorem led to the ring of integers modulo n, the multiplicative group of integers modulo n, and the more general concepts of cyclic groups and abelian groups. Klein's 1872 Erlangen program studied geometry and led to symmetry groups such as the Euclidean group and the group of projective transformations. In 1874 Lie introduced the theory of Lie groups, aiming for "the Galois theory of differential equations". In 1876 Poincaré and Klein introduced the group of Möbius transformations, and its subgroups such as the modular group and Fuchsian group, based on work on automorphic functions in analysis.[10] The abstract concept of group emerged slowly over the middle of the nineteenth century. Galois in 1832 was the first to use the term "group",[11] signifying a collection of permutations closed under composition.[12] Arthur Cayley's 1854 paper On the theory of groups defined a group as a set with an associative composition operation and the identity 1, today called a monoid.[13] In 1870 Kronecker defined an abstract binary operation that was closed, commutative, associative, and had the left cancellation property b ≠ c → a ⋅ b ≠ a ⋅ c {\displaystyle b\neq c\to a\cdot b\neq a\cdot c} ,[14] similar to the modern laws for a finite abelian group.[15] Weber's 1882 definition of a group was a closed binary operation that was associative and had left and right cancellation.[16] Walther von Dyck in 1882 was the first to require inverse elements as part of the definition of a group.[17] Once this abstract group concept emerged, results were reformulated in this abstract setting. For example, Sylow's theorem was reproven by Frobenius in 1887 directly from the laws of a finite group, although Frobenius remarked that the theorem followed from Cauchy's theorem on permutation groups and the fact that every finite group is a subgroup of a permutation group.[18][19] Otto Hölder was particularly prolific in this area, defining quotient groups in 1889, group automorphisms in 1893, as well as simple groups. He also completed the Jordan–Hölder theorem. Dedekind and Miller independently characterized Hamiltonian groups and introduced the notion of the commutator of two elements. Burnside, Frobenius, and Molien created the representation theory of finite groups at the end of the nineteenth century.[18] J. A. de Séguier's 1905 monograph Elements of the Theory of Abstract Groups presented many of these results in an abstract, general form, relegating "concrete" groups to an appendix, although it was limited to finite groups. The first monograph on both finite and infinite abstract groups was O. K. Schmidt's 1916 Abstract Theory of Groups.[20] Early ring theory[edit] See also: Ring theory § History, and Ring (mathematics) § History Noncommutative ring theory began with extensions of the complex numbers to hypercomplex numbers, specifically William Rowan Hamilton's quaternions in 1843. Many other number systems followed shortly. In 1844, Hamilton presented biquaternions, Cayley introduced octonions, and Grassman introduced exterior algebras.[21] James Cockle presented tessarines in 1848[22] and coquaternions in 1849.[23] William Kingdon Clifford introduced split-biquaternions in 1873. In addition Cayley introduced group algebras over the real and complex numbers in 1854 and square matrices in two papers of 1855 and 1858.[24] Once there were sufficient examples, it remained to classify them. In an 1870 monograph, Benjamin Peirce classified the more than 150 hypercomplex number systems of dimension below 6, and gave an explicit definition of an associative algebra. He defined nilpotent and idempotent elements and proved that any algebra contains one or the other. He also defined the Peirce decomposition. Frobenius in 1878 and Charles Sanders Peirce in 1881 independently proved that the only finite-dimensional division algebras over R {\displaystyle \mathbb {R} } were the real numbers, the complex numbers, and the quaternions. In the 1880s Killing and Cartan showed that semisimple Lie algebras could be decomposed into simple ones, and classified all simple Lie algebras. Inspired by this, in the 1890s Cartan, Frobenius, and Molien proved (independently) that a finite-dimensional associative algebra over R {\displaystyle \mathbb {R} } or C {\displaystyle \mathbb {C} } uniquely decomposes into the direct sums of a nilpotent algebra and a semisimple algebra that is the product of some number of simple algebras, square matrices over division algebras. Cartan was the first to define concepts such as direct sum and simple algebra, and these concepts proved quite influential. In 1907 Wedderburn extended Cartan's results to an arbitrary field, in what are now called the Wedderburn principal theorem and Artin–Wedderburn theorem.[25] For commutative rings, several areas together led to commutative ring theory.[26] In two papers in 1828 and 1832, Gauss formulated the Gaussian integers and showed that they form a unique factorization domain (UFD) and proved the biquadratic reciprocity law. Jacobi and Eisenstein at around the same time proved a cubic reciprocity law for the Eisenstein integers.[25] The study of Fermat's last theorem led to the algebraic integers. In 1847, Gabriel Lamé thought he had proven FLT, but his proof was faulty as he assumed all the cyclotomic fields were UFDs, yet as Kummer pointed out, Q ( ζ 23 ) ) {\displaystyle \mat contentType 24 text/html; charset=UTF-8 url 50 https://en.wikipedia.org:443/wiki/Abstract_algebra responseCode 3 200 