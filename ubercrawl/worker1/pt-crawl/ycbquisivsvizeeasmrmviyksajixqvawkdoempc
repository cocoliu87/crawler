ycbquisivsvizeeasmrmviyksajixqvawkdoempc length 6 414463 page 10063 Algebraic object with geometric applications For other uses, see Tensor (disambiguation). This article is about tensors on a single vector space. It is not to be confused with Vector field or Tensor field. The second-order Cauchy stress tensor T {\displaystyle \mathbf {T} } describes the stress experienced by a material at a given point. For any unit vector v {\displaystyle \mathbf {v} } , the product T ⋅ v {\displaystyle \mathbf {T} \cdot \mathbf {v} } is a vector, denoted T ( v ) {\displaystyle \mathbf {T} (\mathbf {v} )} , that quantifies the force per area along the plane perpendicular to v {\displaystyle \mathbf {v} } . This image shows, for cube faces perpendicular to e 1 , e 2 , e 3 {\displaystyle \mathbf {e} _{1},\mathbf {e} _{2},\mathbf {e} _{3}} , the corresponding stress vectors T ( e 1 ) , T ( e 2 ) , T ( e 3 ) {\displaystyle \mathbf {T} (\mathbf {e} _{1}),\mathbf {T} (\mathbf {e} _{2}),\mathbf {T} (\mathbf {e} _{3})} along those faces. Because the stress tensor takes one vector as input and gives one vector as output, it is a second-order tensor. In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors. There are many types of tensors, including scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system; those components form an array, which can be thought of as a high-dimensional matrix. Tensors have become important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as mechanics (stress, elasticity, fluid mechanics, moment of inertia, ...), electrodynamics (electromagnetic tensor, Maxwell tensor, permittivity, magnetic susceptibility, ...), general relativity (stress–energy tensor, curvature tensor, ...) and others. In applications, it is common to study situations in which a different tensor can occur at each point of an object; for example the stress within an object may vary from one location to another. This leads to the concept of a tensor field. In some areas, tensor fields are so ubiquitous that they are often simply called "tensors". Tullio Levi-Civita and Gregorio Ricci-Curbastro popularised tensors in 1900 – continuing the earlier work of Bernhard Riemann, Elwin Bruno Christoffel, and others – as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.[1] Definition[edit] Although seemingly different, the various approaches to defining tensors describe the same geometric concept using different language and at different levels of abstraction. As multidimensional arrays[edit] A tensor may be represented as a (potentially multidimensional) array. Just as a vector in an n-dimensional space is represented by a one-dimensional array with n components with respect to a given basis, any tensor with respect to a basis is represented by a multidimensional array. For example, a linear operator is represented in a basis as a two-dimensional square n × n array. The numbers in the multidimensional array are known as the components of the tensor. They are denoted by indices giving their position in the array, as subscripts and superscripts, following the symbolic name of the tensor. For example, the components of an order 2 tensor T could be denoted Tij , where i and j are indices running from 1 to n, or also by T i j. Whether an index is displayed as a superscript or subscript depends on the transformation properties of the tensor, described below. Thus while Tij and T i j can both be expressed as n-by-n matrices, and are numerically related via index juggling, the difference in their transformation laws indicates it would be improper to add them together. The total number of indices (m) required to identify each component uniquely is equal to the dimension or the number of ways of an array, which is why an array is sometimes referred to as an m-dimensional array or an m-way array. The total number of indices is also called the order, degree or rank of a tensor,[2][3][4] although the term "rank" generally has another meaning in the context of matrices and tensors. Just as the components of a vector change when we change the basis of the vector space, the components of a tensor also change under such a transformation. Each type of tensor comes equipped with a transformation law that details how the components of the tensor respond to a change of basis. The components of a vector can respond in two distinct ways to a change of basis (see Covariance and contravariance of vectors), where the new basis vectors e ^ i {\displaystyle \mathbf {\hat {e}} _{i}} are expressed in terms of the old basis vectors e j {\displaystyle \mathbf {e} _{j}} as, e ^ i = ∑ j = 1 n e j R i j = e j R i j . {\displaystyle \mathbf {\hat {e}} _{i}=\sum _{j=1}^{n}\mathbf {e} _{j}R_{i}^{j}=\mathbf {e} _{j}R_{i}^{j}.} Here R ji are the entries of the change of basis matrix, and in the rightmost expression the summation sign was suppressed: this is the Einstein summation convention, which will be used throughout this article.[Note 1] The components vi of a column vector v transform with the inverse of the matrix R, v ^ i = ( R − 1 ) j i v j , {\displaystyle {\hat {v}}^{i}=\left(R^{-1}\right)_{j}^{i}v^{j},} where the hat denotes the components in the new basis. This is called a contravariant transformation law, because the vector components transform by the inverse of the change of basis. In contrast, the components, wi, of a covector (or row vector), w, transform with the matrix R itself, w ^ i = w j R i j . {\displaystyle {\hat {w}}_{i}=w_{j}R_{i}^{j}.} This is called a covariant transformation law, because the covector components transform by the same matrix as the change of basis matrix. The components of a more general tensor are transformed by some combination of covariant and contravariant transformations, with one transformation law for each index. If the transformation matrix of an index is the inverse matrix of the basis transformation, then the index is called contravariant and is conventionally denoted with an upper index (superscript). If the transformation matrix of an index is the basis transformation itself, then the index is called covariant and is denoted with a lower index (subscript). As a simple example, the matrix of a linear operator with respect to a basis is a rectangular array T {\displaystyle T} that transforms under a change of basis matrix R = ( R i j ) {\displaystyle R=\left(R_{i}^{j}\right)} by T ^ = R − 1 T R {\displaystyle {\hat {T}}=R^{-1}TR} . For the individual matrix entries, this transformation law has the form T ^ j ′ i ′ = ( R − 1 ) i i ′ T j i R j ′ j {\displaystyle {\hat {T}}_{j'}^{i'}=\left(R^{-1}\right)_{i}^{i'}T_{j}^{i}R_{j'}^{j}} so the tensor corresponding to the matrix of a linear operator has one covariant and one contravariant index: it is of type (1,1). Combinations of covariant and contravariant components with the same index allow us to express geometric invariants. For example, the fact that a vector is the same object in different coordinate systems can be captured by the following equations, using the formulas defined above: v = v ^ i e ^ i = ( ( R − 1 ) j i v j ) ( e k R i k ) = ( ( R − 1 ) j i R i k ) v j e k = δ j k v j e k = v k e k = v i e i {\displaystyle \mathbf {v} ={\hat {v}}^{i}\,\mathbf {\hat {e}} _{i}=\left(\left(R^{-1}\right)_{j}^{i}{v}^{j}\right)\left(\mathbf {e} _{k}R_{i}^{k}\right)=\left(\left(R^{-1}\right)_{j}^{i}R_{i}^{k}\right){v}^{j}\mathbf {e} _{k}=\delta _{j}^{k}{v}^{j}\mathbf {e} _{k}={v}^{k}\,\mathbf {e} _{k}={v}^{i}\,\mathbf {e} _{i}} , where δ j k {\displaystyle \delta _{j}^{k}} is the Kronecker delta, which functions similarly to the identity matrix, and has the effect of renaming indices (j into k in this example). This shows several features of the component notation: the ability to re-arrange terms at will (commutativity), the need to use different indices when working with multiple objects in the same expression, the ability to rename indices, and the manner in which contravariant and covariant tensors combine so that all instances of the transformation matrix and its inverse cancel, so that expressions like v i e i {\displaystyle {v}^{i}\,\mathbf {e} _{i}} can immediately be seen to be geometrically identical in all coordinate systems. Similarly, a linear operator, viewed as a geometric object, does not actually depend on a basis: it is just a linear map that accepts a vector as an argument and produces another vector. The transformation law for how the matrix of components of a linear operator changes with the basis is consistent with the transformation law for a contravariant vector, so that the action of a linear operator on a contravariant vector is represented in coordinates as the matrix product of their respective coordinate representations. That is, the components ( T v ) i {\displaystyle (Tv)^{i}} are given by ( T v ) i = T j i v j {\displaystyle (Tv)^{i}=T_{j}^{i}v^{j}} . These components transform contravariantly, since ( T v ^ ) i ′ = T ^ j ′ i ′ v ^ j ′ = [ ( R − 1 ) i i ′ T j i R j ′ j ] [ ( R − 1 ) k j ′ v k ] = ( R − 1 ) i i ′ ( T v ) i . {\displaystyle \left({\widehat {Tv}}\right)^{i'}={\hat {T}}_{j'}^{i'}{\hat {v}}^{j'}=\left[\left(R^{-1}\right)_{i}^{i'}T_{j}^{i}R_{j'}^{j}\right]\left[\left(R^{-1}\right)_{k}^{j'}v^{k}\right]=\left(R^{-1}\right)_{i}^{i'}(Tv)^{i}.} The transformation law for an order p + q tensor with p contravariant indices and q covariant indices is thus given as, T ^ j 1 ′ , … ,  contentType 24 text/html; charset=UTF-8 url 40 https://en.wikipedia.org:443/wiki/Tensor responseCode 3 200 