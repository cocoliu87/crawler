rasssmxijmdaqesgueaszmdeymnqncxgmksodqla length 6 305348 page 10001 Facts provided or learned about something or someone For other uses, see Information (disambiguation). Communication Portal History General aspects Communication theory Information Semiotics Language Logic Sociology Fields Discourse analysis Linguistics Mass communication Organizational communication Pragmatics Semiotics Sociolinguistics Disciplines Public speaking Discourse Culture Argumentation Persuasion Research Rhetoric Categories Outline v t e Information is an abstract concept that refers to that which has the power to inform. At the most fundamental level, information pertains to the interpretation (perhaps formally) of that which may be sensed, or their abstractions. Any natural process that is not completely random and any observable pattern in any medium can be said to convey some amount of information. Whereas digital signals and other data use discrete signs to convey information, other phenomena and artifacts such as analogue signals, poems, pictures, music or other sounds, and currents convey information in a more continuous form.[1] Information is not knowledge itself, but the meaning that may be derived from a representation through interpretation.[2] The concept of information is relevant or connected to various concepts,[3] including constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, proposition, representation, and entropy. Information is often processed iteratively: Data available at one step are processed into information to be interpreted and processed at the next step. For example, in written text each symbol or letter conveys information relevant to the word it is part of, each word conveys information relevant to the phrase it is part of, each phrase conveys information relevant to the sentence it is part of, and so on until at the final step information is interpreted and becomes knowledge in a given domain. In a digital signal, bits may be interpreted into the symbols, letters, numbers, or structures that convey the information available at the next level up. The key characteristic of information is that it is subject to interpretation and processing. The derivation of information from a signal or message may be thought of as the resolution of ambiguity or uncertainty that arises during the interpretation of patterns within the signal or message.[4] Information may be structured as data. Redundant data can be compressed up to an optimal size, which is the theoretical limit of compression. The information available through a collection of data may be derived by analysis. For example, a restaurant collects data from every customer order. That information may be analyzed to produce knowledge that is put to use when the business subsequently wants to identify the most popular or least popular dish.[5] Information can be transmitted in time, via data storage, and space, via communication and telecommunication.[6] Information is expressed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, all information is always conveyed as the content of a message. Information can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication. The uncertainty of an event is measured by its probability of occurrence. Uncertainty is inversely proportional to the probability of occurrence. Information theory takes advantage of this by concluding that more uncertain events require more information to resolve their uncertainty. The bit is a typical unit of information. It is 'that which reduces uncertainty by half'.[7] Other units such as the nat may be used. For example, the information encoded in one "fair" coin flip is log2(2/1) = 1 bit, and in two fair coin flips is log2(4/1) = 2 bits. A 2011 Science article estimates that 97% of technologically stored information was already in digital bits in 2007 and that the year 2002 was the beginning of the digital age for information storage (with digital storage capacity bypassing analogue for the first time).[8] Exact definition of information and digital application[edit] Information can be defined exactly by set theory: "Information is a selection from the domain of information". The "domain of information" is a set that the sender and receiver of information must know before exchanging information. Digital information, for example, consists of building blocks that are all number sequences. Each number sequence represents a selection from its domain. The sender and receiver of digital information (number sequences) must know the domain and binary format of each number sequence before exchanging information. By defining number sequences online, this would be systematically and universally usable. Before the exchanged digital number sequence, an efficient unique link to its online definition can be set. This online-defined digital information (number sequence) would be globally comparable and globally searchable.[9] Etymology[edit] See also: History of the word and concept "information" The English word "information" comes from Middle French enformacion/informacion/information 'a criminal investigation' and its etymon, Latin informati≈ç(n) 'conception, teaching, creation'.[10] In English, "information" is an uncountable mass noun. Information theory[edit] Main article: Information theory Information theory is the scientific study of the quantification, storage, and communication of information. The field was fundamentally established by the works of Harry Nyquist and Ralph Hartley in the 1920s, and Claude Shannon in the 1940s. The field is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering. A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory, and information-theoretic security. There is another opinion regarding the universal definition of information. It lies in the fact that the concept itself has changed along with the change of various historical epochs, and to find such a definition, it is necessary to find standard features and patterns of this transformation. For example, researchers in the field of information Petrichenko E. A. and Semenova V. G., based on a retrospective analysis of changes in the concept of information, give the following universal definition: "Information is a form of transmission of human experience (knowledge)." In their opinion, the change in the essence of the concept of information occurs after various breakthrough technologies for the transfer of experience (knowledge), i.e. the appearance of writing, the printing press, the first encyclopedias, the telegraph, the development of cybernetics, the creation of a microprocessor, the Internet, smartphones, etc. Each new form of experience transfer is a synthesis of the previous ones. That is why we see such a variety of definitions of information, because, according to the law of dialectics "negation-negation", all previous ideas about information are contained in a "filmed" form and in its modern representation.[11] Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference,[12] cryptography, neurobiology,[13] perception,[14] linguistics, the evolution[15] and function[16] of molecular codes (bioinformatics), thermal physics,[17] quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection,[18] pattern recognition, anomaly detection[19] and even art creation. As sensory input[edit] Often information can be viewed as a type of input to an organism or system. Inputs are of two kinds; some inputs are important to the function of the organism (for example, food) or system (energy) by themselves. In his book Sensory Ecology[20] biophysicist David B. Dusenbery called these causal inputs. Other inputs (information) are important only because they are associated with causal inputs and can be used to predict the occurrence of a causal input at a later time (and perhaps another place). Some information is important because of association with other information but eventually there must be a connection to a causal input. In practice, information is usually carried by weak stimuli that must be detected by specialized sensory systems and amplified by energy inputs before they can be functional to the organism or system. For example, light is mainly (but not only, e.g. plants can grow in the direction of the light source) a causal input to plants but for animals it only provides information. The colored light reflected from a flower is too weak for photosynthesis but the visual system of the bee detects it and the bee's nervous system uses the information to guide the bee to the flower, where the bee often finds nectar or pollen, w contentType 24 text/html; charset=UTF-8 url 45 https://en.wikipedia.org:443/wiki/Information responseCode 3 200 