kaiqeeqmtarayiwmfapqrcjahkkooigkigsmrcqq length 6 334463 page 10049 Number used for counting This article is about "positive integers" and "non-negative integers". For all the numbers ..., −2, −1, 0, 1, 2, ..., see Integer. Example of a natural number: 6. There are 6 apples in this picture and 6 is shown as an arabic numeral. In mathematics, the natural numbers are the numbers 1, 2, 3, etc., possibly including 0 as well. Some definitions, including the standard ISO 80000-2,[1] begin the natural numbers with 0, corresponding to the non-negative integers 0, 1, 2, 3, ..., whereas others start with 1, corresponding to the positive integers 1, 2, 3, ...[2][a] Texts that exclude zero from the natural numbers sometimes refer to the natural numbers together with zero as the whole numbers, while in other writings, that term is used instead for the integers (including negative integers).[4] In common language, particularly in primary school education, natural numbers may be called counting numbers[5] to intuitively exclude the negative integers and zero, and also to contrast the discreteness of counting to the continuity of measurement—a hallmark characteristic of real numbers. The natural numbers can be used for counting (as in "there are six coins on the table"), in which case they serve as cardinal numbers. They may also be used for ordering (as in "this is the third largest city in the country"), in which case they serve as ordinal numbers. Natural numbers are sometimes used as labels—also known as nominal numbers, (e.g. jersey numbers in sports)—which do not have the properties of numbers in a mathematical sense.[3][6] The natural numbers form a set, often symbolized as N {\textstyle \mathbb {N} } . Many other number sets are built by successively extending the set of natural numbers: the integers, by including an additive identity 0 (if not yet in) and an additive inverse −n for each nonzero natural number n; the rational numbers, by including a multiplicative inverse 1 / n {\displaystyle 1/n} for each nonzero integer n (and also the product of these inverses by integers); the real numbers by including the limits of Cauchy sequences[b] of rationals; the complex numbers, by adjoining to the real numbers a square root of −1 (and also the sums and products thereof); and so on.[c][d] This chain of extensions canonically embeds the natural numbers in the other number systems. Properties of the natural numbers, such as divisibility and the distribution of prime numbers, are studied in number theory. Problems concerning counting and ordering, such as partitioning and enumerations, are studied in combinatorics. History[edit] Ancient roots[edit] The Ishango bone (on exhibition at the Royal Belgian Institute of Natural Sciences)[7][8][9] is believed to have been used 20,000 years ago for natural number arithmetic. The most primitive method of representing a natural number is to put down a mark for each object. Later, a set of objects could be tested for equality, excess or shortage—by striking out a mark and removing an object from the set. The first major advance in abstraction was the use of numerals to represent numbers. This allowed systems to be developed for recording large numbers. The ancient Egyptians developed a powerful system of numerals with distinct hieroglyphs for 1, 10, and all powers of 10 up to over 1 million. A stone carving from Karnak, dating back from around 1500 BCE and now at the Louvre in Paris, depicts 276 as 2 hundreds, 7 tens, and 6 ones; and similarly for the number 4,622. The Babylonians had a place-value system based essentially on the numerals for 1 and 10, using base sixty, so that the symbol for sixty was the same as the symbol for one—its value being determined from context.[10] A much later advance was the development of the idea that 0 can be considered as a number, with its own numeral. The use of a 0 digit in place-value notation (within other numbers) dates back as early as 700 BCE by the Babylonians, who omitted such a digit when it would have been the last symbol in the number.[e] The Olmec and Maya civilizations used 0 as a separate number as early as the 1st century BCE, but this usage did not spread beyond Mesoamerica.[12][13] The use of a numeral 0 in modern times originated with the Indian mathematician Brahmagupta in 628 CE. However, 0 had been used as a number in the medieval computus (the calculation of the date of Easter), beginning with Dionysius Exiguus in 525 CE, without being denoted by a numeral. Standard Roman numerals do not have a symbol for 0; instead, nulla (or the genitive form nullae) from nullus, the Latin word for "none", was employed to denote a 0 value.[14] The first systematic study of numbers as abstractions is usually credited to the Greek philosophers Pythagoras and Archimedes. Some Greek mathematicians treated the number 1 differently than larger numbers, sometimes even not as a number at all.[f] Euclid, for example, defined a unit first and then a number as a multitude of units, thus by his definition, a unit is not a number and there are no unique numbers (e.g., any two units from indefinitely many units is a 2).[16] However, in the definition of perfect number which comes shortly afterward, Euclid treats 1 as a number like any other.[17] Independent studies on numbers also occurred at around the same time in India, China, and Mesoamerica.[18] Modern definitions[edit] In 19th century Europe, there was mathematical and philosophical discussion about the exact nature of the natural numbers. Henri Poincaré stated that axioms can only be demonstrated in their finite application, and concluded that it is "the power of the mind" which allows conceiving of the indefinite repetition of the same act.[19] Leopold Kronecker summarized his belief as "God made the integers, all else is the work of man".[g] The constructivists saw a need to improve upon the logical rigor in the foundations of mathematics.[h] In the 1860s, Hermann Grassmann suggested a recursive definition for natural numbers, thus stating they were not really natural—but a consequence of definitions. Later, two classes of such formal definitions were constructed; later still, they were shown to be equivalent in most practical applications. Set-theoretical definitions of natural numbers were initiated by Frege. He initially defined a natural number as the class of all sets that are in one-to-one correspondence with a particular set. However, this definition turned out to lead to paradoxes, including Russell's paradox. To avoid such paradoxes, the formalism was modified so that a natural number is defined as a particular set, and any set that can be put into one-to-one correspondence with that set is said to have that number of elements.[22] The second class of definitions was introduced by Charles Sanders Peirce, refined by Richard Dedekind, and further explored by Giuseppe Peano; this approach is now called Peano arithmetic. It is based on an axiomatization of the properties of ordinal numbers: each natural number has a successor and every non-zero natural number has a unique predecessor. Peano arithmetic is equiconsistent with several weak systems of set theory. One such system is ZFC with the axiom of infinity replaced by its negation.[citation needed] Theorems that can be proved in ZFC but cannot be proved using the Peano Axioms include Goodstein's theorem.[23] With all these definitions, it is convenient to include 0 (corresponding to the empty set) as a natural number. Including 0 is now the common convention among set theorists[24] and logicians.[25] Other mathematicians also include 0,[i] and computer languages often start from zero when enumerating items like loop counters and string- or array-elements.[26][27] On the other hand, many mathematicians have kept the older tradition to take 1 to be the first natural number.[28] Notation[edit] The set of all natural numbers is standardly denoted N or N . {\displaystyle \mathbb {N} .} [3][29] Older texts have occasionally employed J as the symbol for this set.[30] Since natural numbers may contain 0 or not, it may be important to know which version is referred to. This is often specified by the context, but may also be done by using a subscript or a superscript in the notation, such as:[1][31] Naturals without zero: { 1 , 2 , . . . } = N ∗ = N + = N 0 ∖ { 0 } = N 1 {\displaystyle \{1,2,...\}=\mathbb {N} ^{*}=\mathbb {N} ^{+}=\mathbb {N} _{0}\smallsetminus \{0\}=\mathbb {N} _{1}} Naturals with zero: { 0 , 1 , 2 , . . . } = N 0 = N 0 = N ∗ ∪ { 0 } {\displaystyle \;\{0,1,2,...\}=\mathbb {N} _{0}=\mathbb {N} ^{0}=\mathbb {N} ^{*}\cup \{0\}} Alternatively, since the natural numbers naturally form a subset of the integers (often denoted Z {\displaystyle \mathbb {Z} } ), they may be referred to as the positive, or the non-negative integers, respectively.[32] To be unambiguous about whether 0 is included or not, sometimes a superscript " ∗ {\displaystyle *} " or "+" is added in the former case, and a subscript (or superscript) "0" is added in the latter case:[1] { 1 , 2 , 3 , … } = { x ∈ Z : x > 0 } = Z + = Z > 0 {\displaystyle \{1,2,3,\dots \}=\{x\in \mathbb {Z} :x>0\}=\mathbb {Z} ^{+}=\mathbb {Z} _{>0}} { 0 , 1 , 2 , … } = { x ∈ Z : x ≥ 0 } = Z 0 + = Z ≥ 0 {\displaystyle \{0,1,2,\dots \}=\{x\in \mathbb {Z} :x\geq 0\}=\mathbb {Z} _{0}^{+}=\mathbb {Z} _{\geq 0}} Properties[edit] This section uses the convention N = N 0 = N ∗ ∪ { 0 } {\displaystyle \mathbb {N} =\mathbb {N} _{0}=\mathbb {N} ^{*}\cup \{0\}} . Addition[edit] Given the set N {\displaystyle \mathbb {N} } of natural numbers and the successor function S : N → N {\displaystyle S\colon \mathbb {N} \to \mathbb {N} } sending each natural number to the next one, one can define addition of natural numbers recursively by setting a + 0 = a and a + S(b) = S(a + b) for all a, b. Thus, a + 1 = a + S(0) = S(a+0) = S(a), a + 2 = a + S(1) = S(a+1) = S(S(a)), and so on. The algebraic structure ( N , + ) {\displaystyle (\mathbb {N} ,+)} is a commutative monoid with identity el contentType 24 text/html; charset=UTF-8 url 48 https://en.wikipedia.org:443/wiki/Natural_number responseCode 3 200 