copmiizadszipiqssmoagcbkusnklgxqyasqaovo length 6 686585 page 10023 Score from a test designed to assess intelligence "IQ" redirects here. For other uses, see IQ (disambiguation). Intelligence quotient One kind of IQ test item, modelled after items in the Raven's Progressive Matrices test ICD-10-PCS Z01.8 ICD-9-CM 94.01 Part of a series on Psychology Outline History Subfields Basic psychology Abnormal Affective neuroscience Affective science Behavioral genetics Behavioral neuroscience Behaviorism Cognitive/Cognitivism Cognitive neuroscience Social Comparative Cross-cultural Cultural Developmental Differential Ecological Evolutionary Experimental Gestalt Intelligence Mathematical Moral Neuropsychology Perception Personality Positive Psycholinguistics Psychophysiology Quantitative Social Theoretical Applied psychology Anomalistic Applied behavior analysis Assessment Clinical Coaching Community Consumer Counseling Critical Educational Ergonomics Feminist Forensic Health Industrial and organizational Legal Media Medical Military Music Occupational health Pastoral Political Psychometrics Psychotherapy Religion School Sport and exercise Suicidology Systems Traffic Concepts Behavior Behavioral engineering Behavioral genetics Behavioral neuroscience Cognition Competence Consciousness Consumer behavior Emotions Feelings Human factors and ergonomics Intelligence Mind Psychology of religion Psychometrics Lists Counseling topics Disciplines Organizations Outline Psychologists Psychotherapies Research methods Schools of thought Timeline Topics Psychology portal v t e An intelligence quotient (IQ) is a total score derived from a set of standardised tests or subtests designed to assess human intelligence.[1] The abbreviation "IQ" was coined by the psychologist William Stern for the German term Intelligenzquotient, his term for a scoring method for intelligence tests at University of Breslau he advocated in a 1912 book.[2] Historically, IQ was a score obtained by dividing a person's mental age score, obtained by administering an intelligence test, by the person's chronological age, both expressed in terms of years and months. The resulting fraction (quotient) was multiplied by 100 to obtain the IQ score.[3] For modern IQ tests, the raw score is transformed to a normal distribution with mean 100 and standard deviation 15.[4] This results in approximately two-thirds of the population scoring between IQ 85 and IQ 115 and about 2 percent each above 130 and below 70.[5][6] Scores from intelligence tests are estimates of intelligence. Unlike, for example, distance and mass, a concrete measure of intelligence cannot be achieved given the abstract nature of the concept of "intelligence".[7] IQ scores have been shown to be associated with such factors as nutrition,[8][9][10] parental socioeconomic status,[11][12] morbidity and mortality,[13][14] parental social status,[15] and perinatal environment.[16] While the heritability of IQ has been investigated for nearly a century, there is still debate about the significance of heritability estimates[17][18] and the mechanisms of inheritance.[19] IQ scores are used for educational placement, assessment of intellectual disability, and evaluating job applicants. In research contexts, they have been studied as predictors of job performance[20] and income.[21] They are also used to study distributions of psychometric intelligence in populations and the correlations between it and other variables. Raw scores on IQ tests for many populations have been rising at an average rate that scales to three IQ points per decade since the early 20th century, a phenomenon called the Flynn effect. Investigation of different patterns of increases in subtest scores can also inform current research on human intelligence. History[edit] See also: History of the race and intelligence controversy Precursors to IQ testing[edit] Historically, even before IQ tests were devised, there were attempts to classify people into intelligence categories by observing their behavior in daily life.[22][23] Those other forms of behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of "intelligence" used in a particular case and on the reliability and error of estimation in the classification procedure. The English statistician Francis Galton (1822–1911) made the first attempt at creating a standardized test for rating a person's intelligence. A pioneer of psychometrics and the application of statistical methods to the study of human diversity and the study of inheritance of human traits, he believed that intelligence was largely a product of heredity (by which he did not mean genes, although he did develop several pre-Mendelian theories of particulate inheritance).[24][25][26] He hypothesized that there should exist a correlation between intelligence and other observable traits such as reflexes, muscle grip, and head size.[27] He set up the first mental testing center in the world in 1882 and he published "Inquiries into Human Faculty and Its Development" in 1883, in which he set out his theories. After gathering data on a variety of physical variables, he was unable to show any such correlation, and he eventually abandoned this research.[28][29] Psychologist Alfred Binet, co-developer of the Stanford–Binet test French psychologist Alfred Binet, together with Victor Henri and Théodore Simon, had more success in 1905, when they published the Binet–Simon test, which focused on verbal abilities. It was intended to identify "mental retardation" in school children,[30] but in specific contradistinction to claims made by psychiatrists that these children were "sick" (not "slow") and should therefore be removed from school and cared for in asylums.[31] The score on the Binet–Simon scale would reveal the child's mental age. For example, a six-year-old child who passed all the tasks usually passed by six-year-olds—but nothing beyond—would have a mental age that matched his chronological age, 6.0. (Fancher, 1985). Binet thought that intelligence was multifaceted, but came under the control of practical judgment. In Binet's view, there were limitations with the scale and he stressed what he saw as the remarkable diversity of intelligence and the subsequent need to study it using qualitative, as opposed to quantitative, measures (White, 2000). American psychologist Henry H. Goddard published a translation of it in 1910. American psychologist Lewis Terman at Stanford University revised the Binet–Simon scale, which resulted in the Stanford–Binet Intelligence Scales (1916). It became the most popular test in the United States for decades.[30][32][33][34] General factor (g)[edit] Main article: g factor The many different kinds of IQ tests include a wide variety of item content. Some test items are visual, while many are verbal. Test items vary from being based on abstract-reasoning problems to concentrating on arithmetic, vocabulary, or general knowledge. The British psychologist Charles Spearman in 1904 made the first formal factor analysis of correlations between the tests. He observed that children's school grades across seemingly unrelated school subjects were positively correlated, and reasoned that these correlations reflected the influence of an underlying general mental ability that entered into performance on all kinds of mental tests. He suggested that all mental performance could be conceptualized in terms of a single general ability factor and a large number of narrow task-specific ability factors. Spearman named it g for "general factor" and labeled the specific factors or abilities for specific tasks s.[35] In any collection of test items that make up an IQ test, the score that best measures g is the composite score that has the highest correlations with all the item scores. Typically, the "g-loaded" composite score of an IQ test battery appears to involve a common strength in abstract reasoning across the test's item content.[citation needed] United States military selection in World War I[edit] During World War I, the Army needed a way to evaluate and assign recruits to appropriate tasks. This led to the development of several mental tests by Robert Yerkes, who worked with major hereditarians of American psychometrics—including Terman, Goddard—to write the test.[36] The testing generated controversy and much public debate in the United States. Nonverbal or "performance" tests were developed for those who could not speak English or were suspected of malingering.[30] Based on Goddard's translation of the Binet–Simon test, the tests had an impact in screening men for officer training: ...the tests did have a strong impact in some areas, particularly in screening men for officer training. At the start of the war, the army and national guard maintained nine thousand officers. By the end, two hundred thousand officers presided, and two- thirds of them had started their careers in training camps where the tests were applied. In some camps, no man scoring below C could be considered for officer training.[36] In total 1.75 million men were tested, making the results the first mass-produced written tests of intelligence, though considered dubious and non-usable, for reasons including high variability of test implementation throughout different camps and questions testing for familiarity with American culture rather than intelligence.[36] After the war, positive publicity promoted by army psychologists helped to make psychology a respected field.[37] Subsequently, there was an increase in jobs and funding in psychology in the United States.[38] Group intelligence tests were developed and became widely used in schools and industry.[39] The results of these tests, which at the time reaffirmed contemporary racism and nationalism, are considered controversial and dubious, having rested on certain contested assumptions contentType 24 text/html; charset=UTF-8 url 36 https://en.wikipedia.org:443/wiki/IQ responseCode 3 200 